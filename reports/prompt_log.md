# Log of Prompts used in Project

## Step 1: Project Setup

**Prompt:**
Role (Роль):

Дій як Senior MLOps Engineer та Data Scientist. Твоя мета — допомогти мені налаштувати структуру проекту для експериментів з машинним навчанням, дотримуючись принципового підходу "Reproducible Research".

Project Context (Контекст):



Середовище: Локальна розробка (Windows/macOS/Linux), Jupyter Notebook.

Версія Python: 3.10.10.

Інфраструктура: Docker НЕ використовується.

Інструментарій:

MLflow: обов'язково для логування експериментів, параметрів та артефактів.

AutoML: (наприклад, H2O, PyCaret або FLAML) для пошуку базової моделі.

Обмеження ресурсів: Час навчання AutoML обмежено 5 хвилинами (або фіксованою кількістю ітерацій).

Data Description (Опис даних):



Суть задачі: Датасет "Класифікація грибів".

Розмір датасету: 8124 рядків, 23 колонок (ознак)

Цільова змінна: class

Особливості: Сильно переважає ознака "odor", що дозволяє вгадувати з легкістю клас гриба

Task (Завдання):



Структура репозиторію: Запропонуй професійну структуру папок для цього проекту (використовуючи стандартні практики, наприклад, Cookiecutter Data Science, але адаптовану під простіший локальний запуск без Docker).

Конфігурація: Напиши код для файлу configs/main.yaml (використовуючи Hydra або просто YAML), де будуть винесені:

Шляхи до даних (raw, processed).

Параметри для MLflow (tracking URI).

Ліміти для AutoML (time_limit: 300 секунд).

Параметри спліту даних (test_size, random_state).

Стратегія роботи з даними: Коротко опиши, як краще розділити ці конкретні дані (Train/Val/Test), враховуючи описаний мною дисбаланс або специфіку задачі.

**Result:**
Вітаю. Як Senior MLOps Engineer, я підготував для тебе структуру та конфігурацію, яка забезпечить відтворюваність (reproducibility) експериментів та зручність роботи в локальному середовищі без зайвого оверхеду (як-от Docker), але з професійним підходом.

Оскільки ми маємо справу з датасетом "Гриби" (Mushroom Data Set), де є "читерська" ознака (odor), ми врахуємо це в стратегії.

1. Структура Репозиторію
Ми використаємо адаптовану версію Cookiecutter Data Science. Вона спрощена (прибрано Docker-файли, cloud-конфіги), але чітко розділяє код, дані та конфігурацію.

Plaintext

mushroom-classification/
├── configs/                 # Конфігураційні файли (Hydra/YAML)
│   └── main.yaml            # Головний конфіг
├── data/                    # Дані (ігноруються git, окрім, можливо, sample)
│   ├── raw/                 # Вихідний датасет (immutable - ніколи не змінюємо)
│   └── processed/           # Очищені дані, готові для AutoML (train/test спліти)
├── notebooks/               # Jupyter ноутбуки
│   ├── 01_eda.ipynb         # Аналіз даних (Exploratory Data Analysis)
│   └── 02_baseline.ipynb    # Запуск AutoML експерименту
├── mlruns/                  # Локальне сховище MLflow (створюється автоматично)
├── src/                     # Вихідний код проекту
│   ├── __init__.py
│   ├── data_loader.py       # Скрипти для завантаження та спліту даних
│   └── train.py             # Скрипт запуску AutoML (якщо запускати не з ноутбука)
├── .gitignore               # Ігнорування data/, mlruns/, .env
├── requirements.txt         # Фіксація версій бібліотек (критично для відтворюваності)
└── README.md                # Опис проекту та інструкція запуску
2. Конфігурація (configs/main.yaml)
Для керування конфігурацією я рекомендую використовувати Hydra (або просто читати цей YAML через pyyaml). Це дозволяє легко змінювати параметри без редагування коду.

Створи файл configs/main.yaml:

YAML

project:
  name: "mushroom_classification_automl"
  description: "Predicting mushroom edibility with AutoML and MLflow tracking"
  seed: 42  # Глобальний random_state для повної відтворюваності

paths:
  raw_data: "data/raw/mushrooms.csv"
  processed_data: "data/processed/"
  output_dir: "outputs/"

mlflow:
  tracking_uri: "file:./mlruns"  # Локальне збереження
  experiment_name: "mushroom_automl_baseline"

data:
  target_column: "class"
  test_size: 0.2
  stratify: true                 # Критично важливо для задач класифікації

automl:
  engine: "flaml"                # Рекомендую FLAML або H2O для швидких результатів
  time_budget: 300               # 300 секунд (5 хвилин), як вказано в умовах
  metric: "accuracy"             # Або 'roc_auc', якщо класи незбалансовані
  task: "classification"
  
feature_engineering:
  drop_features: []              # Сюди можна додати "odor", щоб ускладнити задачу
3. Стратегія роботи з даними (Data Strategy)
Враховуючи специфіку датасету грибів (Mushroom Dataset), пропоную такий підхід:

А. Розбиття (Splitting Strategy)
Оскільки датасет невеликий (8124 рядків), нам не потрібен окремий Validation set у вигляді файлу, тому що AutoML фреймворки (H2O, FLAML) використовують крос-валідацію (Cross-Validation) всередині процесу навчання.

Train / Test Split:

Пропорція: 80% Train (на цьому вчиться AutoML), 20% Test (Holdout — відкладена вибірка для фінальної перевірки).

Stratification (Стратифікація): Обов'язково. Цільова змінна class (їстівний/отруйний) має бути розподілена в train і test у тій самій пропорції, що й у вихідних даних.

Random State: Фіксуємо (наприклад, 42), щоб спліт завжди був однаковим.

Б. Проблема ознаки "Odor" (Data Leakage / Triviality)
Ти вірно зауважив про ознаку odor (запах). У цьому датасеті вона має майже 100% кореляцію з цільовою змінною.

Ризик: Модель просто запам'ятає: "неприємний запах = отруйний". Це дасть 100% точність, але модель буде марною, якщо у нас не буде даних про запах (наприклад, класифікація за фото).

Рекомендація для експериментів: Завдяки MLflow ми можемо провести два окремих "Run" (запуски):

Experiment A (Baseline): Всі ознаки включені. Очікуємо метрику ~1.0 (100%). Це перевірка пайплайну.

Experiment B (Realistic): Виключити колонку odor. Це покаже реальну здатність моделі розрізняти гриби за візуальними ознаками (форма шапки, колір тощо).

В. Preprocessing (Обробка)
Оскільки ми використовуємо AutoML:

Encoding: Більшість AutoML (H2O, PyCaret) самі вміють кодувати категоріальні змінні. Оскільки датасет грибів повністю категоріальний, краще залишити дані як є (strings), або використати OrdinalEncoder, якщо обраний AutoML (наприклад, старі версії sklearn-based) цього вимагає.

Missing Values: У цьому датасеті stalk-root має багато пропусків (позначених як ?). AutoML чудово з цим впорається сам, не потрібно заповнювати їх вручну.

## Step 2: Baseline Model

**Prompt:**
Context Update (Оновлення контексту):

Ми успішно створили структуру папок і конфіг. Тепер переходимо до етапу Baseline Modeling.

Strategic Decision (Важливо):

Ми вирішили відмовитися від Experiment A (з усіма ознаками) і одразу переходити до Experiment B.



Причина: Ознака odor є "лікуючою" (data leakage/trivial feature). Вона дає тривіальну класифікацію.

Вимога: Код має видаляти колонку odor перед навчанням, щоб модель вчилася на складніших візуальних ознаках.

Task (Завдання):

Напиши код для файлу src/train_baseline.py (або скрипт для ноутбука notebooks/02_baseline.ipynb), який реалізує наступне:



Model: Використовуй RandomForestClassifier як надійний baseline для табличних даних.

Configuration: Зчитуй параметри (шляхи, гіперпараметри моделі) через hydra з configs/main.yaml.

MLflow Tracking (Обов'язково):

Ініціалізуй експеримент.

Залогуй параметри моделі (n_estimators, max_depth тощо).

Залогуй метрики: Accuracy, Precision, Recall, F1-score (macro).

Artifacts (Візуалізація):

Побудуй Confusion Matrix, збережи її як .png і залогуй в MLflow як артефакт.

Оскільки це бінарна класифікація, побудуй та залогуй ROC-curve та Precision-Recall curve.

Output: Код має бути готовим до копіювання і запуску.


**Result:**
Чудовий стратегічний хід. Видалення ознаки odor перетворює цю задачу з тривіальної "перевірки правил" на реальну задачу розпізнавання патернів (pattern recognition), де модель мусить аналізувати форму шапки, колір спор та середовище існування.

Ось професійна реалізація src/train_baseline.py. Я використав scikit-learn Pipeline для чистоти коду та OrdinalEncoder, оскільки Random Forest добре працює з категоріями, закодованими числами, і це ефективніше за OneHotEncoding для цього типу дерев.

1. Оновлення конфігурації (configs/main.yaml)
Спочатку додамо секцію для Random Forest та список колонок для видалення у ваш YAML файл.

YAML

project:
  name: "mushroom_classification_baseline"
  seed: 42

paths:
  raw_data: "data/raw/mushrooms.csv"
  # Використовуємо гідру, тому шляхи будуть відносні або абсолютні
  # Для скрипта нижче ми використаємо hydra.utils.to_absolute_path

mlflow:
  tracking_uri: "file:./mlruns"
  experiment_name: "mushroom_rf_no_odor" # Окремий експеримент для чесних даних

data:
  target_column: "class"
  drop_columns: ["odor"] # ВАЖЛИВО: Видаляємо "лікуючу" ознаку
  test_size: 0.2

# Гіперпараметри для Baseline моделі
model:
  n_estimators: 100
  max_depth: 10
  min_samples_split: 5
  class_weight: "balanced" # Корисно, якщо класи все ж трохи нерівні
2. Скрипт тренування (src/train_baseline.py)
Цей скрипт повністю автономний. Він завантажує дані, обробляє їх, тренує модель і відправляє всі звіти в MLflow.

Як запустити
Переконайся, що ти знаходишся в корені проекту.

Виконай команду в терміналі:

Bash

python src/train_baseline.py
Після завершення, запусти UI, щоб побачити результати:

Bash

mlflow ui
Що робить цей код (Senior Insights)
Robust Preprocessing: Використання Pipeline гарантує, що трансформації (OrdinalEncoding), вивчені на Train, будуть коректно застосовані до Test (і до майбутніх нових даних). Це запобігає Data Leakage на етапі препроцесингу.

Гнучкість Hydra: Якщо ти захочеш змінити глибину дерева, тобі не треба лізти в код. Просто напиши в терміналі: python src/train_baseline.py model.max_depth=20.

Visual Analytics: В MLOps сухі цифри метрик часто оманливі. Графіки ROC та PR Curve (збережені в MLflow) дадуть тобі миттєве розуміння поведінки моделі на різних порогах ймовірності (probability thresholds).

## Step 3: AutoML Integration

**Prompt:**

Контекст:

Ми завершили етап Baseline. Скрипт src/train_baseline.py успішно навчає Random Forest без колонки odor. Ми досягли високої точності, використовуючи лише візуальні ознаки (gill-color, spore-print-color).

Завдання (AutoML):

Мені потрібен скрипт src/train_automl.py.

Вимоги до коду:



Використовуй бібліотеку FLAML (Fast Lightweight AutoML).

Конфігурація: Зчитуй ліміт часу (time_budget) та інші параметри з нашого configs/main.yaml.

Preprocessing: Обов'язково реалізуй видалення колонки odor (так само, як в baseline).

MLflow:

Логуй параметри найкращої моделі.

Логуй метрики (accuracy, f1, roc_auc).

Збережи найкращу модель як артефакт MLflow.

Output: Виведи в консоль, який алгоритм переміг (наприклад, XGBoost, LGBM чи ExtraTrees) і скільки часу з
Це логічний наступний крок. Ми перевірили гіпотезу про те, що задача вирішується (Baseline), тепер дозволимо AutoML знайти найефективніший спосіб це зробити (найшвидший інференс, найменша модель або найкраща метрика за обмежений час).

Для цього ми використаємо FLAML (від Microsoft). Це чудовий вибір для локальної розробки, бо він дуже економний до ресурсів і швидко знаходить сильні моделі (LightGBM, XGBoost, CatBoost).

1. Підготовка оточення
Перед запуском переконайся, що FLAML встановлено:

Bash

pip install flaml
2. Скрипт src/train_automl.py
Цей код повністю інтегрований з вашим configs/main.yaml.

Що очікувати (Senior Insights)
Winner Algorithm: FLAML, швидше за все, обере LGBM (LightGBM) або XGBoost. Для табличних даних такого розміру ці алгоритми зазвичай перемагають Random Forest як за точністю, так і за швидкістю інференсу (передбачення).

Time Budget: Оскільки датасет маленький, FLAML може знайти ідеальну модель набагато швидше, ніж за 300 секунд (можливо, за 10-20 секунд). Він використає решту часу, щоб спробувати "відшліфувати" результат або спробувати простіші моделі.

Порівняння з Baseline: Ви зможете відкрити MLflow UI і порівняти два рани:

mushroom_rf_no_odor (ваш ручний Random Forest)

mushroom_automl_baseline_automl (цей запуск)

Шукайте не тільки різницю в accuracy (вона може бути однаковою — 1.0), але й в складності моделі. Часто AutoML знаходить меншу модель (менше дерев, менша глибина), яка працює так само добре.

## Step 4: Iterative Process

**Prompt:**

Context:

Ми завершили технічну частину.

У нас пройшли 3 ітерації:



Baseline (Random Forest) -> Acc 1.0.

AutoML (Budget 60s) -> Acc 1.0 (XGBoost).

AutoML (Budget 10s) -> Acc 1.0 (Test performed manually to check efficiency).

Task 1: Requirements

Напиши вміст файлу requirements.txt. Включи туди: pandas, numpy, scikit-learn, mlflow, flaml, hydra-core, matplotlib, seaborn.

Task 2: README.md (Step 6)

Напиши професійний README.md для GitHub українською мовою.

Структура:



Назва проекту: Mushroom Classification (Baseline vs AutoML).

Опис: Порівняння підходів для класифікації грибів з урахуванням MLOps (видалення data leakage 'odor').

Встановлення: pip install -r requirements.txt.

Запуск:

Baseline: python src/train_baseline.py

AutoML: python src/train_automl.py

MLflow: Інструкція як запустити mlflow ui.

Результати: Коротка табличка, що обидві моделі досягли 100%, але AutoML (XGBoost) налаштовується автоматично.

Task 3: Опис ітерацій (для звіту Step 5)

Коротко опиши логіку наших ітерацій для звіту:



Ітерація 1: Перевірка гіпотези (RF).

Ітерація 2: Пошук оптимальної моделі (AutoML 60s).

Ітерація 3: Оптимізація ресурсів (AutoML 10s - спроба отримати той же результат швидше).

**Result:**

Ми застосували підхід "від простого до складного", розбивши розробку на три стратегічні етапи:

* **Ітерація 1: Baseline (Sanity Check)**
    * **Мета:** Перевірити, чи можливо взагалі класифікувати гриби *без* використання колонки `odor` (запах), яка викликає витік даних.
    * **Інструмент:** Random Forest (ручні налаштування).
    * **Результат:** Accuracy 1.0. Гіпотеза підтверджена: візуальні ознаки (колір спор, форма шапки) містять достатньо інформації для безпомилкової класифікації.

* **Ітерація 2: AutoML Exploration (Time Budget: 60s)**
    * **Мета:** Знайти архітектуру моделі, яка є більш ефективною для продакшну, ніж Random Forest, без ручного перебору параметрів.
    * **Інструмент:** FLAML.
    * **Результат:** Знайдено **XGBoost**. Точність залишилась 1.0, але процес пошуку був повністю автоматизованим. Це гарантує, що ми не пропустили кращий алгоритм.

* **Ітерація 3: Resource Optimization (Time Budget: 10s)**
    * **Мета:** Перевірити чутливість AutoML до обмеження ресурсів ("Cost Efficiency").
    * **Результат:** Навіть за 10 секунд FLAML встиг знайти модель з Accuracy 1.0. Це свідчить про те, що для даного датасету бюджет у 60 секунд був надлишковим, і в реальному хмарному середовищі можна суттєво зекономити обчислювальні ресурси.айняв пошук.

**Result:**
